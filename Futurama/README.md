# Slurm notes
## Installation
Install slurm with command

    sudo apt install slurmd slurmctld
Next, we need to create the `slurm.conf` file which configures how your slurm queue is set up. Here we use a very simple one: (please adjust the COMPUTE NODES section to your machines specs. e.g. if you have10 cores `CPUs=10` and your memory is 32000MB `RealMemory=32000` .

    sudo chmod 777 /etc/slurm-llnl

    sudo cat << EOF > /etc/slurm-llnl/slurm.conf  
    # slurm.conf file generated by configurator.html.  
    # Put this file on all nodes of your cluster.  
    # See the slurm.conf man page for more information.  
    #  
    ClusterName=localcluster  
    SlurmctldHost=localhost  
    MpiDefault=none  
    ProctrackType=proctrack/linuxproc  
    ReturnToService=2  
    SlurmctldPidFile=/var/run/slurmctld.pid  
    SlurmctldPort=6817  
    SlurmdPidFile=/var/run/slurmd.pid  
    SlurmdPort=6818  
    SlurmdSpoolDir=/var/lib/slurm-llnl/slurmd  
    SlurmUser=slurm  
    StateSaveLocation=/var/lib/slurm-llnl/slurmctld  
    SwitchType=switch/none  
    TaskPlugin=task/none  
    #  
    # TIMERS  
    InactiveLimit=0  
    KillWait=30  
    MinJobAge=300  
    SlurmctldTimeout=120  
    SlurmdTimeout=300  
    Waittime=0  
    # SCHEDULING  
    SchedulerType=sched/backfill  
    SelectType=select/cons_tres  
    SelectTypeParameters=CR_Core  
    #  
    #AccountingStoragePort=  
    AccountingStorageType=accounting_storage/none  
    JobCompType=jobcomp/none  
    JobAcctGatherFrequency=30  
    JobAcctGatherType=jobacct_gather/none  
    SlurmctldDebug=info  
    SlurmctldLogFile=/var/log/slurm-llnl/slurmctld.log  
    SlurmdDebug=info  
    SlurmdLogFile=/var/log/slurm-llnl/slurmd.log  
    #  
    # COMPUTE NODES  
    NodeName=localhost CPUs=1 RealMemory=500 State=UNKNOWN  
    PartitionName=LocalQ Nodes=ALL Default=YES MaxTime=INFINITE State=UP  
    EOF
    
    sudo chmod 755 /etc/slurm-llnl/
Now lets gets slurm started with systemd:

    sudo systemctl start slurmctld  
    sudo systemctl start slurmd
Lastly, let's set our machine as idle:

    sudo scontrol update nodename=localhost state=idle  
and now we can start queuing up jobs:

    sinfo  
    PARTITION AVAIL TIMELIMIT NODES STATE NODELIST  
    LocalQ* up infinite 1 idle localhost
Now you have a working slurm queue, if you need to make changes to your config edit the `slurm.conf` and simply restart `slurmctld` and `slurmd` via systemd.

## Usage examples
The best way to understand how to use the Slurm directives to allocate the required resources for your job is to provide some examples based on the type of workload you plan to run:

### SINGLE CPU (NON-PARALLEL) JOBS

Single CPU, or non-parallel, jobs are often simple commands that can be run on a single vCPU. You can request a single CPU job as follows:

     #!/bin/bash
    
    #SBATCH --job-name=singlecpu
    #SBATCH --ntasks=1
    #SBATCH --cpus-per-task=1
    
    # Your script goes here
    sleep 30
    echo "hello"
  If you have many single CPU commands that you want to run at the same time, rather than consecutively on the same vCPU, you can specify a greater number of tasks and then use `srun` to allocate each command to a task. In the below example, one task will echo "I'm task 1" and the other task will echo "I'm task 2" but both of these will be executed simultaneously on two different vCPUs.

    #!/bin/bash
    
    #SBATCH --job-name=singlecputasks
    #SBATCH --ntasks=2
    #SBATCH --cpus-per-task=1
    
    # Your script goes here
    srun --ntasks=1 echo "I'm task 1"
    srun --ntasks=1 echo "I'm task 2"
  **Note**: In the example above, because we did not specify anything about nodes, Slurm may allocate the tasks to two different nodes depending on where there is a free vCPU available. See the MPI Jobs section below if you would like to see how to specify how tasks are allocated to nodes.
  ### MULTITHREADED (SYMMETRIC MULTIPROCESSING - SMP) JOBS

If your workload can be multithreaded, i.e. run across multiple vCPUs on the same node (symmetric multiprocessing), you should request a single node and increase the  `--cpus-per-task`  directive to be equal to the required number of threads:

    #!/bin/bash
    
    #SBATCH --job-name=multithreaded
    #SBATCH --nodes=1
    #SBATCH --ntasks=1
    #SBATCH --cpus-per-task=8
    
    # Your script goes here
    mycommand --threads 8
**Note**: The specified `--cpus-per-task` must be equal to or less than the number of vCPUs available on a single compute nodes, otherwise the allocation will fail. You can see how many vCPUs each of your compute nodes have in the information tab of your cluster in RONIN:
You can also specify multiple tasks if you have a number of multithreaded commands you wish to run in parallel. Just ensure you also specify enough nodes for the tasks to run across, or omit the nodes directive and let Slurm determine how many nodes are necessary. For example, the below script will launch 4 tasks in total, each on a separate node, with 4 vCPUs allocated to each task:

    #!/bin/bash
    
    #SBATCH --job-name=multithreadedtasks
    #SBATCH --nodes=4
    #SBATCH --ntasks=4
    #SBATCH --cpus-per-task=4
    
    # Your script goes here
    srun --ntasks=1 mycommand1 --threads 4
    srun --ntasks=1 mycommand2 --threads 4
    srun --ntasks=1 mycommand3 --threads 4
    srun --ntasks=1 mycommand4 --threads 4
### MESSAGE PASSING INTERFACE (MPI) JOBS

MPI jobs can be run across multiple nodes and the  `--ntasks`  option is used to specify the number of vCPUs (since 1 vCPU per task is the default). For example, if you want your MPI job to run across 16 vCPUs and do not care how many nodes these CPUs are split across:

    #!/bin/bash
    
    #SBATCH --job-name=simplempi
    #SBATCH --ntasks=16
    
    # Your script goes here
    mpirun myscript
If instead you want your 16 vCPU job to run across 2 nodes, using 8 vCPUs per node you can add the `--ntasks-per-node` flag:
If instead you want your 16 vCPU job to run across 2 nodes, using 8 vCPUs per node you can add the `--ntasks-per-node` flag:
<!--stackedit_data:
eyJoaXN0b3J5IjpbOTMyNDk4NjAxLC0xNzI0MjY1OTUwXX0=
-->