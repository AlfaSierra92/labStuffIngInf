# Slurm notes
## Installation
Install slurm with command

    sudo apt install slurmd slurmctld
Next, we need to create the `slurm.conf` file which configures how your slurm queue is set up. Here we use a very simple one: (please adjust the COMPUTE NODES section to your machines specs. e.g. if you have10 cores `CPUs=10` and your memory is 32000MB `RealMemory=32000` .

    sudo chmod 777 /etc/slurm-llnl

    sudo cat << EOF > /etc/slurm-llnl/slurm.conf  
    # slurm.conf file generated by configurator.html.  
    # Put this file on all nodes of your cluster.  
    # See the slurm.conf man page for more information.  
    #  
    ClusterName=localcluster  
    SlurmctldHost=localhost  
    MpiDefault=none  
    ProctrackType=proctrack/linuxproc  
    ReturnToService=2  
    SlurmctldPidFile=/var/run/slurmctld.pid  
    SlurmctldPort=6817  
    SlurmdPidFile=/var/run/slurmd.pid  
    SlurmdPort=6818  
    SlurmdSpoolDir=/var/lib/slurm-llnl/slurmd  
    SlurmUser=slurm  
    StateSaveLocation=/var/lib/slurm-llnl/slurmctld  
    SwitchType=switch/none  
    TaskPlugin=task/none  
    #  
    # TIMERS  
    InactiveLimit=0  
    KillWait=30  
    MinJobAge=300  
    SlurmctldTimeout=120  
    SlurmdTimeout=300  
    Waittime=0  
    # SCHEDULING  
    SchedulerType=sched/backfill  
    SelectType=select/cons_tres  
    SelectTypeParameters=CR_Core  
    #  
    #AccountingStoragePort=  
    AccountingStorageType=accounting_storage/none  
    JobCompType=jobcomp/none  
    JobAcctGatherFrequency=30  
    JobAcctGatherType=jobacct_gather/none  
    SlurmctldDebug=info  
    SlurmctldLogFile=/var/log/slurm-llnl/slurmctld.log  
    SlurmdDebug=info  
    SlurmdLogFile=/var/log/slurm-llnl/slurmd.log  
    #  
    # COMPUTE NODES  
    NodeName=localhost CPUs=1 RealMemory=512 State=UNKNOWN  
    PartitionName=LocalQ Nodes=ALL Default=YES MaxTime=INFINITE State=UP  
    EOF
    
    sudo chmod 755 /etc/slurm-llnl/
Now lets gets slurm started with systemd:

    sudo systemctl start slurmctld  
    sudo systemctl start slurmd
Lastly, let's set our machine as idle:

    sudo scontrol update nodename=localhost state=idle  
and now we can start queuing up jobs:

    sinfo  
    PARTITION AVAIL TIMELIMIT NODES STATE NODELIST  
    LocalQ* up infinite 1 idle localhost
Now you have a working slurm queue, if you need to make changes to your config edit the `slurm.conf` and simply restart `slurmctld` and `slurmd` via systemd.

## Usage examples
The best way to understand how to use the Slurm directives to allocate the required resources for your job is to provide some examples based on the type of workload you plan to run:

### SINGLE CPU (NON-PARALLEL) JOBS

Single CPU, or non-parallel, jobs are often simple commands that can be run on a single vCPU. You can request a single CPU job as follows:

     #!/bin/bash
    
    #SBATCH --job-name=singlecpu
    #SBATCH --ntasks=1
    #SBATCH --cpus-per-task=1
    
    # Your script goes here
    sleep 30
    echo "hello"
  If you have many single CPU commands that you want to run at the same time, rather than consecutively on the same vCPU, you can specify a greater number of tasks and then use `srun` to allocate each command to a task. In the below example, one task will echo "I'm task 1" and the other task will echo "I'm task 2" but both of these will be executed simultaneously on two different vCPUs.

    #!/bin/bash
    
    #SBATCH --job-name=singlecputasks
    #SBATCH --ntasks=2
    #SBATCH --cpus-per-task=1
    
    # Your script goes here
    srun --ntasks=1 echo "I'm task 1"
    srun --ntasks=1 echo "I'm task 2"
  **Note**: In the example above, because we did not specify anything about nodes, Slurm may allocate the tasks to two different nodes depending on where there is a free vCPU available. See the MPI Jobs section below if you would like to see how to specify how tasks are allocated to nodes.
  ### MULTITHREADED (SYMMETRIC MULTIPROCESSING - SMP) JOBS

**If your workload can be multithreaded**, i.e. run across multiple vCPUs on the same node (symmetric multiprocessing), you should request a single node and increase the  `--cpus-per-task`  directive to be equal to the required number of threads:

    #!/bin/bash
    
    #SBATCH --job-name=multithreaded
    #SBATCH --nodes=1 ##number of server used
    #SBATCH --ntasks=1 ##
    #SBATCH --cpus-per-task=8
    
    # Your script goes here
    mycommand --threads 8
**Note**: The specified `--cpus-per-task` must be equal to or less than the number of vCPUs available on a single compute nodes, otherwise the allocation will fail. You can see how many vCPUs each of your compute nodes have in the information tab of your cluster in RONIN:
You can also specify multiple tasks if you have a number of multithreaded commands you wish to run in parallel. Just ensure you also specify enough nodes for the tasks to run across, or omit the nodes directive and let Slurm determine how many nodes are necessary. For example, the below script will launch 4 tasks in total, each on a separate node, with 4 vCPUs allocated to each task:

    #!/bin/bash
    
    #SBATCH --job-name=multithreadedtasks
    #SBATCH --nodes=4
    #SBATCH --ntasks=4
    #SBATCH --cpus-per-task=4
    
    # Your script goes here
    srun --ntasks=1 mycommand1 --threads 4
    srun --ntasks=1 mycommand2 --threads 4
    srun --ntasks=1 mycommand3 --threads 4
    srun --ntasks=1 mycommand4 --threads 4
### MESSAGE PASSING INTERFACE (MPI) JOBS

MPI jobs can be run across multiple nodes and the  `--ntasks`  option is used to specify the number of vCPUs (since 1 vCPU per task is the default). For example, if you want your MPI job to run across 16 vCPUs and do not care how many nodes these CPUs are split across:

    #!/bin/bash
    
    #SBATCH --job-name=simplempi
    #SBATCH --ntasks=16
    
    # Your script goes here
    mpirun myscript
If instead you want your 16 vCPU job to run across 2 nodes, using 8 vCPUs per node you can add the `--ntasks-per-node` flag:

    #!/bin/bash
    
    #SBATCH --job-name=nodempi
    #SBATCH --ntasks=16
    #SBATCH --ntasks-per-node=8
    
    # Your script goes here
    mpirun myscript
### SLURM COMMANDS

So once you've got your head around all the SLURM directives and you have your SLURM script ready to go, you may be wondering "What do I do with it now?". Fortunately the process of submitting your job and monitoring its progress is pretty simple. You just need to remember a few different SLURM commands.

To submit your SLURM job to the queue, use the  `sbatch`  command:

 `sbatch myslurmscript.sh`

You will then be given a message with the ID for that job:

 `Submitted batch job 208`

In this example, the job ID is 208.

To check the status of this job in the queue, use the  `squeue`  command:

`squeue --job 208`

**Note**: You can run  `squeue`  without the  `--job`  flag to check the status for all jobs in the queue.

To cancel this job, use  `scancel`:

 `scancel 208`

To check the status of your entire cluster, use  `sinfo`. This can sometimes be helpful for troubleshooting if your jobs aren't running as expected. For more information on  `sinfo`  see  [here](https://slurm.schedmd.com/sinfo.html).
## sbatch vs srun
The documentation says

```
srun is used to submit a job for execution in real time
```

while
```
sbatch is used to submit a job script for later execution.
```
They both accept practically the same set of parameters. The main difference is that  `srun`  is *interactive and blocking* (you get the result in your terminal and you cannot write other commands until it is finished), while  `sbatch`  is *batch processing and non-blocking* (results are written to a file and you can submit other commands right away).

If you use  `srun`  in the background with the  `&`  sign, then you remove the 'blocking' feature of  `srun`, which becomes interactive but non-blocking. It is still interactive though, meaning that the output will clutter your terminal, and the  `srun`  processes are linked to your terminal. If you disconnect, you will loose control over them, or they might be killed (depending on whether they use  `stdout`  or not basically). And they will be killed if the machine to which you connect to submit jobs is rebooted.

If you use  `sbatch`, you submit your job and it is handled by Slurm ; you can disconnect, kill your terminal, etc. with no consequence. Your job is no longer linked to a running process.

> What are some things that I can do with one that I cannot do with the other, and why?

A feature that is available to  `sbatch`  and not to  `srun`  is  [job arrays](https://slurm.schedmd.com/job_array.html). As  `srun`  can be used within an  `sbatch`  script, there is nothing that you cannot do with  `sbatch`.

> How are these related to each other, and how do they differ for srun vs sbatch?

All the parameters  `--ntasks`,  `--nodes`,  `--cpus-per-task`,  `--ntasks-per-node`  have the same meaning in both commands. That is true for nearly all parameters, with the notable exception of  `--exclusive`.

> What is happening "under the hood" that causes this to be the case?

`srun`  **immediately executes** the script on the remote host, while  `sbatch`  **copies the script in an internal storage and then uploads it** on the compute node when the job starts. You can check this by modifying your submission script after it has been submitted; changes will not be taken into account (see  [this](https://stackoverflow.com/questions/32216228/after-submitting-a-m-batch-job-with-slurm-can-i-edit-my-m-file-without-changi)).

> How do they interact with each other, and what is the "canonical" use-case for each of them?

You typically use  `sbatch`  to submit a job and  `srun`  in the submission script to create job steps as Slurm calls them.  `srun`  is used to launch the processes. If your program is a parallel MPI program,  `srun`  takes care of creating all the MPI processes. If not,  `srun`  will run your program as many times as specified by the  `--ntasks`  option. There are many use cases depending on whether your program is paralleled or not, has a long-running time or not, is composed of a single executable or not, etc. Unless otherwise specified,  `srun`  inherits by default the pertinent options of the  `sbatch`  or  `salloc`  which it runs under (from  [here](https://groups.google.com/forum/#!topic/slurm-devel/wKaUEOzuQq4)).

> Specifically, would I ever use srun by itself?

Other than for small tests, no. A common use is  `srun --pty bash`  to get a shell on a compute job.
<!--stackedit_data:
eyJoaXN0b3J5IjpbODIzNzA2MzMyLDE5NDM2ODEyNDQsMTY5MD
Q1Mzk4NiwtOTk5NDgxNTI2LDE5OTI4MjIyNDksLTE3MjQyNjU5
NTBdfQ==
-->