# Slurm notes
## Installation
Install slurm with command

    sudo apt install slurmd slurmctld
Next, we need to create the `slurm.conf` file which configures how your slurm queue is set up. Here we use a very simple one: (please adjust the COMPUTE NODES section to your machines specs. e.g. if you have10 cores `CPUs=10` and your memory is 32000MB `RealMemory=32000` .

    sudo chmod 777 /etc/slurm-llnl

    sudo cat << EOF > /etc/slurm-llnl/slurm.conf  
    # slurm.conf file generated by configurator.html.  
    # Put this file on all nodes of your cluster.  
    # See the slurm.conf man page for more information.  
    #  
    ClusterName=localcluster  
    SlurmctldHost=localhost  
    MpiDefault=none  
    ProctrackType=proctrack/linuxproc  
    ReturnToService=2  
    SlurmctldPidFile=/var/run/slurmctld.pid  
    SlurmctldPort=6817  
    SlurmdPidFile=/var/run/slurmd.pid  
    SlurmdPort=6818  
    SlurmdSpoolDir=/var/lib/slurm-llnl/slurmd  
    SlurmUser=slurm  
    StateSaveLocation=/var/lib/slurm-llnl/slurmctld  
    SwitchType=switch/none  
    TaskPlugin=task/none  
    #  
    # TIMERS  
    InactiveLimit=0  
    KillWait=30  
    MinJobAge=300  
    SlurmctldTimeout=120  
    SlurmdTimeout=300  
    Waittime=0  
    # SCHEDULING  
    SchedulerType=sched/backfill  
    SelectType=select/cons_tres  
    SelectTypeParameters=CR_Core  
    #  
    #AccountingStoragePort=  
    AccountingStorageType=accounting_storage/none  
    JobCompType=jobcomp/none  
    JobAcctGatherFrequency=30  
    JobAcctGatherType=jobacct_gather/none  
    SlurmctldDebug=info  
    SlurmctldLogFile=/var/log/slurm-llnl/slurmctld.log  
    SlurmdDebug=info  
    SlurmdLogFile=/var/log/slurm-llnl/slurmd.log  
    #  
    # COMPUTE NODES  
    NodeName=localhost CPUs=1 RealMemory=500 State=UNKNOWN  
    PartitionName=LocalQ Nodes=ALL Default=YES MaxTime=INFINITE State=UP  
    EOF
    
    sudo chmod 755 /etc/slurm-llnl/
Now lets gets slurm started with systemd:

    sudo systemctl start slurmctld  
    sudo systemctl start slurmd
Lastly, let's set our machine as idle:

    sudo scontrol update nodename=localhost state=idle  
and now we can start queuing up jobs:

    sinfo  
    PARTITION AVAIL TIMELIMIT NODES STATE NODELIST  
    LocalQ* up infinite 1 idle localhost
Now you have a working slurm queue, if you need to make changes to your config edit the `slurm.conf` and simply restart `slurmctld` and `slurmd` via systemd.

## Usage examples
The best way to understand how to use the Slurm directives to allocate the required resources for your job is to provide some examples based on the type of workload you plan to run:

### SINGLE CPU (NON-PARALLEL) JOBS

Single CPU, or non-parallel, jobs are often simple commands that can be run on a single vCPU. You can request a single CPU job as follows:

     #!/bin/bash
    
    #SBATCH --job-name=singlecpu
    #SBATCH --ntasks=1
    #SBATCH --cpus-per-task=1
    
    # Your script goes here
    sleep 30
    echo "hello"
  If you have many single CPU commands that you want to run at the same time, rather than consecutively on the same vCPU, you can specify a greater number of tasks and then use `srun` to allocate each command to a task. In the below example, one task will echo "I'm task 1" and the other task will echo "I'm task 2" but both of these will be executed simultaneously on two different vCPUs.

<!--stackedit_data:
eyJoaXN0b3J5IjpbMzg0NjYyMjA0LC0xNzI0MjY1OTUwXX0=
-->